{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query decomposition**  is a strategy to improve question-answering using the power of LLM by re-writing and rephrasing the user input to improve retrieval process by breaking down a question into well-written sub-questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain langchain-community langchain-core langchain-openai langchain-text-splitters openai bs4 chromadb python-dotenv colorama tqdm tiktoken httplib2 langchainhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore\n",
    "\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Pairing and format Q and A\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"{Fore.GREEN}Question: {question}{Fore.RESET}\\n{Fore.WHITE}Answer: {answer}\\n\\n {Fore.RESET}\"\n",
    "    print(\"=====  QUESTION/ANSWER PAIRS: =====\")\n",
    "    print(formatted_string.strip())\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from operator import itemgetter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from colorama import Fore\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents(documents):\n",
    "    # Index and load embeddings\n",
    "    vectorstore = Chroma.from_documents(documents=documents, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "    # Create the vector store\n",
    "    return vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DECOMPOSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant trained to generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answered in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  generate_sub_questions(query):\n",
    "    generate_queries_decomposition = (\n",
    "        prompt_decomposition \n",
    "        | llm \n",
    "        | StrOutputParser()\n",
    "        | (lambda x: x.split(\"\\n\"))\n",
    "    ) \n",
    "\n",
    "    # Run\n",
    "    sub_questions = generate_queries_decomposition.invoke({\"question\": query})\n",
    "    questions_str = \"\\n\".join(sub_questions)\n",
    "    print(Fore.MAGENTA + \"=====  SUBQUESTIONS: =====\" + Fore.RESET)\n",
    "    print(Fore.WHITE + questions_str + Fore.RESET + \"\\n\") \n",
    "    return sub_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ANSWER SUBQUESTIONS RECURSIVELY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {sub_question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {sub_question}\n",
    "\"\"\"\n",
    "prompt_qa = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs(retriever, sub_questions):\n",
    "    \"\"\" ask the LLM to generate a pair of question and answer based on the original user query \"\"\"\n",
    "    q_a_pairs = \"\"\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "        # chain\n",
    "        generate_qa = (\n",
    "            {\"context\": itemgetter(\"sub_question\") | retriever, \"sub_question\": itemgetter(\"sub_question\"), \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "            | prompt_qa \n",
    "            | llm \n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        answer = generate_qa.invoke({\"sub_question\": sub_question, \"q_a_pairs\": q_a_pairs})\n",
    "        q_a_pair = format_qa_pair(sub_question, answer)\n",
    "        q_a_pairs = q_a_pairs + \"\\n --- \\n\" + q_a_pair \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ANSWER INDIVIDUALY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rag(retriever, sub_questions):\n",
    "    rag_results = []\n",
    "    for sub_question in sub_questions:\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "\n",
    "        answer_chain = (\n",
    "            prompt_rag\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        answer = answer_chain.invoke({\"question\": sub_question, \"context\": retrieved_docs})\n",
    "        rag_results.append(answer)\n",
    "\n",
    "    return rag_results, sub_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httplib2\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "http = httplib2.Http()\n",
    "\n",
    "def get_links(url):\n",
    "    status, response = http.request(url)\n",
    "    links = []\n",
    "    for link in BeautifulSoup(response, parse_only=SoupStrainer('a')):\n",
    "        if link.has_attr('href'):\n",
    "            links.append(f\"https://www.paulgraham.com/{link.attrs['href']}\")\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "import bs4\n",
    "\n",
    "url = 'https://www.paulgraham.com/articles.html'\n",
    "\n",
    "links = get_links(url)\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=list(links),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"table\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "raw_text = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SUMMARIZE AND ANSWER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "Paul Graham is known for his influential essays on startups, technology, programming, \n",
    "and life in general. He is a co-founder of Y Combinator, a prominent startup accelerator, \n",
    "and has written numerous essays that have inspired many entrepreneurs and technologists.\n",
    "You assist users with general inquiries and {question} based on {context} /\n",
    "\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(question):\n",
    "    retriever = index_documents(raw_text)\n",
    "    sub_questions = generate_sub_questions(query)\n",
    "    generate_qa_pairs(retriever,  sub_questions)\n",
    "    answers, questions = retrieve_and_rag(retriever, sub_questions)\n",
    "    context = format_qa_pairs(questions, answers)\n",
    "\n",
    "    final_rag_chain = (\n",
    "        {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "        | chat_prompt_template\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return final_rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#8e44ad\">Try it Out!</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query(\"how do I write Python?\")\n",
    "print(f\"{Fore.GREEN}{response}{Fore.RESET}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
